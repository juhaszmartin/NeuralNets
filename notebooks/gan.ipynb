{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dfd897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Path to the info.txt file in the jp2k folder\n",
    "\n",
    "info_path = r\"c:\\matek_msc\\Neural Nets\\data\\LIVE\\jp2k\\info.txt\"\n",
    "\n",
    "# Set your base directory where ALL images are stored\n",
    "\n",
    "IMAGE_DIR = r\"C:\\matek_msc\\Neural Nets\\data\\LIVE\\jp2k\"  # All .bmp files are here\n",
    "\n",
    "\n",
    "def load_info_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads the info.txt file and groups entries by their reference image name.\n",
    "    For each reference name (e.g. buildings.bmp), if an entry has a noise value of 0,\n",
    "    it is considered the original (y) and all others (nonzero) are treated as distortions (X).\n",
    "    \"\"\"\n",
    "    groups = {}\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Skip empty lines or comments\n",
    "            if not line or line.startswith(\"//\"):\n",
    "                continue\n",
    "\n",
    "            # Each line is expected to have three tokens: <ref_image> <distorted_image> <value>\n",
    "            tokens = line.split()\n",
    "\n",
    "            if len(tokens) != 3:\n",
    "                continue  # skip unexpected format lines\n",
    "\n",
    "            ref_img, distorted_img, value_str = tokens\n",
    "\n",
    "            try:\n",
    "                value = float(value_str)\n",
    "\n",
    "            except ValueError:\n",
    "                continue  # skip if value is not convertible\n",
    "\n",
    "            if ref_img not in groups:\n",
    "                groups[ref_img] = {\"original\": None, \"distorted\": []}\n",
    "\n",
    "            if value == 0:\n",
    "\n",
    "                # if multiple originals exist, warn and override.\n",
    "                if groups[ref_img][\"original\"] is not None:\n",
    "                    print(\n",
    "                        f\"Warning: Multiple originals for {ref_img}. Overwriting previous original {groups[ref_img]['original']} with new original {distorted_img}.\"\n",
    "                    )\n",
    "                groups[ref_img][\"original\"] = distorted_img\n",
    "\n",
    "            else:\n",
    "                groups[ref_img][\"distorted\"].append((distorted_img, value))\n",
    "    return groups\n",
    "\n",
    "\n",
    "grouped_pairs = load_info_file(info_path)\n",
    "\n",
    "\n",
    "# Print out the groups\n",
    "for ref, pair in grouped_pairs.items():\n",
    "    print(f\"Reference image (key): {ref}\")\n",
    "    original = pair[\"original\"]\n",
    "    distorted = pair[\"distorted\"]\n",
    "\n",
    "    if original:\n",
    "        print(\"  Original (y):\", original)\n",
    "    else:\n",
    "        print(\"  Warning: No original found!\")\n",
    "\n",
    "    if distorted:\n",
    "        print(\"  Distorted (X):\")\n",
    "        for d, val in distorted:\n",
    "            print(f\"    {d} with noise value {val}\")\n",
    "\n",
    "    else:\n",
    "        print(\"  No distorted versions found!\")\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def load_image(filename):\n",
    "    \"\"\"Load and preprocess an image from the IMAGE_DIR.\"\"\"\n",
    "    if not filename:\n",
    "        return None\n",
    "\n",
    "    path = os.path.join(IMAGE_DIR, filename)\n",
    "\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "\n",
    "        img = img.resize((256, 256))  # Resize to 256x256\n",
    "        return np.array(img, dtype=np.float32) / 255.0  # Normalize to [0,1]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filename}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Organize data\n",
    "X = []  # Distorted images\n",
    "y = []  # Original images\n",
    "\n",
    "\n",
    "for ref, pair in grouped_pairs.items():\n",
    "    original_filename = pair[\"original\"]\n",
    "    distorted_list = pair[\"distorted\"]\n",
    "\n",
    "    # Load original image\n",
    "    orig_img = load_image(original_filename)\n",
    "\n",
    "    if orig_img is None:\n",
    "        print(f\"⚠️ Original image missing: {original_filename}\")\n",
    "        continue\n",
    "\n",
    "    # Load distorted versions\n",
    "\n",
    "    if not distorted_list:\n",
    "        print(f\"⚠️ No distorted versions for {original_filename}\")\n",
    "        continue\n",
    "\n",
    "    for distorted_filename, noise_val in distorted_list:\n",
    "        dist_img = load_image(distorted_filename)\n",
    "\n",
    "        if dist_img is None:\n",
    "            continue  # Skip if distorted image fails to load\n",
    "\n",
    "        X.append(dist_img)\n",
    "        y.append(orig_img)\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(f\"✅ Training data shape: {X_train.shape} (Distorted: X)\")\n",
    "print(f\"✅ Test data shape: {X_test.shape} (Original clean: y)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160acb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.models import Model as KModel\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# === Generator ===\n",
    "def build_generator(input_shape=(256, 256, 3)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    e1 = layers.Conv2D(64, 4, strides=2, padding=\"same\")(inputs)\n",
    "    e2 = layers.Conv2D(128, 4, strides=2, padding=\"same\")(e1)\n",
    "    e3 = layers.Conv2D(256, 4, strides=2, padding=\"same\")(e2)\n",
    "    e4 = layers.Conv2D(512, 4, strides=2, padding=\"same\")(e3)\n",
    "\n",
    "    def res_block(x, filters):\n",
    "        shortcut = x\n",
    "        x = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "        x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.Add()([shortcut, x])\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        return x\n",
    "\n",
    "    b = res_block(e4, 512)\n",
    "    b = res_block(b, 512)\n",
    "    d1 = layers.Conv2DTranspose(256, 4, strides=2, padding=\"same\", activation=\"relu\")(b)\n",
    "    d1 = layers.Concatenate()([d1, e3])\n",
    "    d2 = layers.Conv2DTranspose(128, 4, strides=2, padding=\"same\", activation=\"relu\")(d1)\n",
    "    d2 = layers.Concatenate()([d2, e2])\n",
    "    d3 = layers.Conv2DTranspose(64, 4, strides=2, padding=\"same\", activation=\"relu\")(d2)\n",
    "    d3 = layers.Concatenate()([d3, e1])\n",
    "    outputs = layers.Conv2DTranspose(3, 4, strides=2, padding=\"same\", activation=\"sigmoid\")(d3)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "\n",
    "# === Discriminator ===\n",
    "def build_discriminator(input_shape=(256, 256, 3)):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    tar = layers.Input(shape=input_shape)\n",
    "    x = layers.Concatenate()([inp, tar])\n",
    "    x = layers.Conv2D(64, 4, strides=2, padding=\"same\")(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Conv2D(128, 4, strides=2, padding=\"same\")(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Conv2D(256, 4, strides=2, padding=\"same\")(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Conv2D(512, 4, strides=2, padding=\"same\")(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Conv2D(1, 4, strides=1, padding=\"same\")(x)\n",
    "    return Model([inp, tar], x)\n",
    "\n",
    "\n",
    "# === Perceptual Loss ===\n",
    "class PerceptualLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, layer=\"block3_conv3\"):\n",
    "        super().__init__()\n",
    "        vgg = VGG19(include_top=False, weights=\"imagenet\")\n",
    "        self.model = KModel(inputs=vgg.input, outputs=vgg.get_layer(layer).output)\n",
    "        self.model.trainable = False\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.keras.applications.vgg19.preprocess_input(y_true * 255.0)\n",
    "        y_pred = tf.keras.applications.vgg19.preprocess_input(y_pred * 255.0)\n",
    "        return tf.reduce_mean(tf.abs(self.model(y_true) - self.model(y_pred)))\n",
    "\n",
    "\n",
    "# === Data Augmentation ===\n",
    "def data_augment(x, y):\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        x = tf.image.flip_left_right(x)\n",
    "        y = tf.image.flip_left_right(y)\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        x = tf.image.flip_up_down(x)\n",
    "        y = tf.image.flip_up_down(y)\n",
    "    k = tf.random.uniform((), minval=0, maxval=4, dtype=tf.int32)\n",
    "    x = tf.image.rot90(x, k)\n",
    "    y = tf.image.rot90(y, k)\n",
    "    x = tf.image.random_brightness(x, max_delta=0.1)\n",
    "    x = tf.image.random_contrast(x, lower=0.9, upper=1.1)\n",
    "    noise = tf.random.normal(shape=tf.shape(x), mean=0.0, stddev=0.02)\n",
    "    x = tf.clip_by_value(x + noise, 0.0, 1.0)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# === Losses and optimizers ===\n",
    "l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "percep_loss = PerceptualLoss()\n",
    "adv_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "gen_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "disc_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "# === Instantiate models ===\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "\n",
    "# === Training step ===\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    real_labels = tf.ones((x.shape[0], 16, 16, 1))\n",
    "    fake_labels = tf.zeros_like(real_labels)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        fake_y = generator(x, training=True)\n",
    "        disc_real = discriminator([x, y], training=True)\n",
    "        disc_fake = discriminator([x, fake_y], training=True)\n",
    "        d_loss_real = adv_loss(real_labels, disc_real)\n",
    "        d_loss_fake = adv_loss(fake_labels, disc_fake)\n",
    "        d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "        g_l1 = l1_loss(y, fake_y)\n",
    "        g_perc = percep_loss(y, fake_y)\n",
    "        g_adv = adv_loss(real_labels, disc_fake)\n",
    "        g_ssim = ssim_loss(y, fake_y)\n",
    "\n",
    "        # Total generator loss\n",
    "        g_loss = g_l1 + 0.5 * g_perc + 0.5 * g_ssim + 0.1 * g_adv\n",
    "    grads_gen = tape.gradient(g_loss, generator.trainable_variables)\n",
    "    grads_disc = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    gen_opt.apply_gradients(zip(grads_gen, generator.trainable_variables))\n",
    "    disc_opt.apply_gradients(zip(grads_disc, discriminator.trainable_variables))\n",
    "    return g_loss, d_loss\n",
    "\n",
    "\n",
    "# === Pretraining step ===\n",
    "@tf.function\n",
    "def pretrain_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = generator(x, training=True)\n",
    "        loss = l1_loss(y, y_pred)\n",
    "    grads = tape.gradient(loss, generator.trainable_variables)\n",
    "    gen_opt.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# === Evaluation ===\n",
    "def evaluate_model(generator, X_test, y_test, sample_size=5):\n",
    "    idx = np.random.choice(len(X_test), sample_size, replace=False)\n",
    "    x_sample = X_test[idx]\n",
    "    y_sample = y_test[idx]\n",
    "    y_pred = generator.predict(x_sample, verbose=0)\n",
    "    psnr_scores = [psnr(y_sample[i], y_pred[i], data_range=1.0) for i in range(sample_size)]\n",
    "    ssim_scores = [ssim(y_sample[i], y_pred[i], channel_axis=-1, data_range=1.0) for i in range(sample_size)]\n",
    "    return np.mean(psnr_scores), np.mean(ssim_scores)\n",
    "\n",
    "\n",
    "# === Training loop ===\n",
    "def train_model(X_train, y_train, X_test, y_test, batch_size=8, pretrain_epochs=5, gan_epochs=50):\n",
    "    train_size = X_train.shape[0]\n",
    "    steps_per_epoch = train_size // batch_size\n",
    "    best_psnr = 0.0\n",
    "\n",
    "    for epoch in range(pretrain_epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(0, train_size, batch_size):\n",
    "            x_batch = X_train[i : i + batch_size]\n",
    "            y_batch = y_train[i : i + batch_size]\n",
    "            x_aug, y_aug = zip(*[data_augment(tf.convert_to_tensor(x), tf.convert_to_tensor(y)) for x, y in zip(x_batch, y_batch)])\n",
    "            x_aug = tf.stack(x_aug)\n",
    "            y_aug = tf.stack(y_aug)\n",
    "            loss = pretrain_step(x_aug, y_aug)\n",
    "            total_loss += loss\n",
    "        print(f\"[Pretrain] Epoch {epoch+1}/{pretrain_epochs} - L1 Loss: {total_loss / steps_per_epoch:.4f}\")\n",
    "\n",
    "    for epoch in range(gan_epochs):\n",
    "        g_total, d_total = 0, 0\n",
    "        for i in range(0, train_size, batch_size):\n",
    "            x_batch = X_train[i : i + batch_size]\n",
    "            y_batch = y_train[i : i + batch_size]\n",
    "            x_aug, y_aug = zip(*[data_augment(tf.convert_to_tensor(x), tf.convert_to_tensor(y)) for x, y in zip(x_batch, y_batch)])\n",
    "            x_aug = tf.stack(x_aug)\n",
    "            y_aug = tf.stack(y_aug)\n",
    "            g_loss, d_loss = train_step(x_aug, y_aug)\n",
    "            g_total += g_loss\n",
    "            d_total += d_loss\n",
    "\n",
    "        avg_g = g_total / steps_per_epoch\n",
    "        avg_d = d_total / steps_per_epoch\n",
    "        print(f\"[GAN] Epoch {epoch+1}/{gan_epochs} - G Loss: {avg_g:.4f}, D Loss: {avg_d:.4f}\")\n",
    "\n",
    "        avg_psnr, avg_ssim = evaluate_model(generator, X_test, y_test)\n",
    "        print(f\"[Eval] PSNR: {avg_psnr:.2f}, SSIM: {avg_ssim:.3f}\")\n",
    "\n",
    "        if avg_psnr > best_psnr:\n",
    "            best_psnr = avg_psnr\n",
    "            generator.save(\"best_denoiser2.h5\")\n",
    "            print(f\"✅ Best model saved with PSNR: {best_psnr:.2f}\")\n",
    "\n",
    "\n",
    "train_model(X_train, y_train, X_test, y_test, batch_size=8, pretrain_epochs=20, gan_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best generator\n",
    "best_generator = tf.keras.models.load_model(\"best_denoiser.h5\")\n",
    "\n",
    "# Predict on test set\n",
    "denoised_test = best_generator.predict(X_test, verbose=0)\n",
    "\n",
    "# Baseline metrics (noisy vs. clean)\n",
    "baseline_psnr = [psnr(y_test[i], X_test[i], data_range=1) for i in range(len(X_test))]\n",
    "baseline_ssim = [ssim(y_test[i], X_test[i], data_range=1, channel_axis=-1) for i in range(len(X_test))]\n",
    "\n",
    "# Denoised metrics (denoised vs. clean)\n",
    "psnr_scores = [psnr(y_test[i], denoised_test[i], data_range=1) for i in range(len(X_test))]\n",
    "ssim_scores = [ssim(y_test[i], denoised_test[i], data_range=1, channel_axis=-1) for i in range(len(X_test))]\n",
    "\n",
    "# Print averages\n",
    "print(\"=== Average Metrics ===\")\n",
    "print(f\"Baseline PSNR:   {np.mean(baseline_psnr):.2f} ± {np.std(baseline_psnr):.2f}\")\n",
    "print(f\"Denoised PSNR:   {np.mean(psnr_scores):.2f} ± {np.std(psnr_scores):.2f}\")\n",
    "print()\n",
    "print(f\"Baseline SSIM:   {np.mean(baseline_ssim):.2f} ± {np.std(baseline_ssim):.2f}\")\n",
    "print(f\"Denoised SSIM:   {np.mean(ssim_scores):.2f} ± {np.std(ssim_scores):.2f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i in range(3):\n",
    "    # Noisy\n",
    "    plt.subplot(3, 4, i * 4 + 1)\n",
    "    plt.imshow(X_test[i])\n",
    "    plt.title(\"Noisy Input\\nPSNR: {:.2f}\".format(baseline_psnr[i]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Denoised\n",
    "    plt.subplot(3, 4, i * 4 + 2)\n",
    "    plt.imshow(denoised_test[i])\n",
    "    plt.title(f\"Denoised\\nPSNR: {psnr_scores[i]:.2f}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Clean\n",
    "    plt.subplot(3, 4, i * 4 + 3)\n",
    "    plt.imshow(y_test[i])\n",
    "    plt.title(\"Original Clean\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Error map (absolute difference)\n",
    "    plt.subplot(3, 4, i * 4 + 4)\n",
    "    error_map = np.abs(denoised_test[i] - y_test[i])\n",
    "    plt.imshow(error_map, cmap=\"hot\")\n",
    "    plt.title(\"Error Map\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
